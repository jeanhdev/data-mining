{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient\n",
    "Dans ce notebook, vous allez programmer l'algorithme de descente de gradient dans sa version ordinaire, stochastique et mini-batch.\n",
    "Dans un premier temps, vous utiliserez un jeu de données simple pour tester votre code dans le cas d'une régressison linéaire. Dans un second temps, vous implémenterz vos algorithmes sur un jeu de donnée plus conséquent et comparerez vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un jeu de données simple pour tester votre code\n",
    "Un je de donnée simple constitué de 100 instance est créé artificiellement à l'aide de la relation linéaire $\\ y_{i} = 2+5\\times x_{i}  $<br>\n",
    "Ce jeu de donnée ne comporte qu'une seuler variables $\\ x  $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un vecteur X de taille 100 \n",
    "X=np.arange(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Affichez X\n",
    "print(X)\n",
    "# Affichez ses dimensions\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de Y\n",
    "Y=2+5*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   7  12  17  22  27  32  37  42  47  52  57  62  67  72  77  82  87\n",
      "  92  97 102 107 112 117 122 127 132 137 142 147 152 157 162 167 172 177\n",
      " 182 187 192 197 202 207 212 217 222 227 232 237 242 247 252 257 262 267\n",
      " 272 277 282 287 292 297 302 307 312 317 322 327 332 337 342 347 352 357\n",
      " 362 367 372 377 382 387 392 397 402 407 412 417 422 427 432 437 442 447\n",
      " 452 457 462 467 472 477 482 487 492 497]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de Y\n",
    "print(Y)\n",
    "# Affichez les dimensions de Y\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons notre jeu de données avec les valeurs $\\ x_{i} $ et leur label $\\ y_{i} $ correspondant, il faut transformer le vecteur $\\ X $ poour la faire correpondre à une matrice de forme générale :\n",
    "\n",
    "$$\\begin{bmatrix} 1 & x_{11} & x_{12} & ... & x_{1j} & ... & x_{1n}\\\\ 1 & x_{21} & x_{22} & ... & x_{2j} & ... & x_{2n} \\\\...&...&...&...&...&...&... \\\\1 & x_{i1} & x_{i2} & ... & x_{ij} & ... & x_{in}\\\\...&...&...&...&...&...&...\\\\1 & x_{m1} & x_{m2} & ... & x_{mj} & ... & x_{mn} \\end{bmatrix}$$ <br>\n",
    "Nous réaliserons cette transformation à l'aide d'une fonction ***Stand_Trans***  qui prend en entrée la matrice $\\ X $ à transformer, le nombre de d'instances $\\ m $ et le nombre de variables $\\ n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stand_Trans(X,m,n):\n",
    "    # Transorme la matrice en entrée en une matrice de dimension m*n\n",
    "    X=X.reshape(m,n) \n",
    "    # Crée une matrice de dimension m*1 remplie de 1\n",
    "    Ones=np.ones((m)).reshape(m,1) \n",
    "    # Concatenation horizontale des matrice X et Ones pour donner lieu à une nouvelle matrice X\n",
    "    X=np.hstack([Ones,X]) \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquez la fonction Stand_Trans au vecteur X\n",
    "X=Stand_Trans(X,100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  2.]\n",
      " [ 1.  3.]\n",
      " [ 1.  4.]\n",
      " [ 1.  5.]\n",
      " [ 1.  6.]\n",
      " [ 1.  7.]\n",
      " [ 1.  8.]\n",
      " [ 1.  9.]\n",
      " [ 1. 10.]\n",
      " [ 1. 11.]\n",
      " [ 1. 12.]\n",
      " [ 1. 13.]\n",
      " [ 1. 14.]\n",
      " [ 1. 15.]\n",
      " [ 1. 16.]\n",
      " [ 1. 17.]\n",
      " [ 1. 18.]\n",
      " [ 1. 19.]\n",
      " [ 1. 20.]\n",
      " [ 1. 21.]\n",
      " [ 1. 22.]\n",
      " [ 1. 23.]\n",
      " [ 1. 24.]\n",
      " [ 1. 25.]\n",
      " [ 1. 26.]\n",
      " [ 1. 27.]\n",
      " [ 1. 28.]\n",
      " [ 1. 29.]\n",
      " [ 1. 30.]\n",
      " [ 1. 31.]\n",
      " [ 1. 32.]\n",
      " [ 1. 33.]\n",
      " [ 1. 34.]\n",
      " [ 1. 35.]\n",
      " [ 1. 36.]\n",
      " [ 1. 37.]\n",
      " [ 1. 38.]\n",
      " [ 1. 39.]\n",
      " [ 1. 40.]\n",
      " [ 1. 41.]\n",
      " [ 1. 42.]\n",
      " [ 1. 43.]\n",
      " [ 1. 44.]\n",
      " [ 1. 45.]\n",
      " [ 1. 46.]\n",
      " [ 1. 47.]\n",
      " [ 1. 48.]\n",
      " [ 1. 49.]\n",
      " [ 1. 50.]\n",
      " [ 1. 51.]\n",
      " [ 1. 52.]\n",
      " [ 1. 53.]\n",
      " [ 1. 54.]\n",
      " [ 1. 55.]\n",
      " [ 1. 56.]\n",
      " [ 1. 57.]\n",
      " [ 1. 58.]\n",
      " [ 1. 59.]\n",
      " [ 1. 60.]\n",
      " [ 1. 61.]\n",
      " [ 1. 62.]\n",
      " [ 1. 63.]\n",
      " [ 1. 64.]\n",
      " [ 1. 65.]\n",
      " [ 1. 66.]\n",
      " [ 1. 67.]\n",
      " [ 1. 68.]\n",
      " [ 1. 69.]\n",
      " [ 1. 70.]\n",
      " [ 1. 71.]\n",
      " [ 1. 72.]\n",
      " [ 1. 73.]\n",
      " [ 1. 74.]\n",
      " [ 1. 75.]\n",
      " [ 1. 76.]\n",
      " [ 1. 77.]\n",
      " [ 1. 78.]\n",
      " [ 1. 79.]\n",
      " [ 1. 80.]\n",
      " [ 1. 81.]\n",
      " [ 1. 82.]\n",
      " [ 1. 83.]\n",
      " [ 1. 84.]\n",
      " [ 1. 85.]\n",
      " [ 1. 86.]\n",
      " [ 1. 87.]\n",
      " [ 1. 88.]\n",
      " [ 1. 89.]\n",
      " [ 1. 90.]\n",
      " [ 1. 91.]\n",
      " [ 1. 92.]\n",
      " [ 1. 93.]\n",
      " [ 1. 94.]\n",
      " [ 1. 95.]\n",
      " [ 1. 96.]\n",
      " [ 1. 97.]\n",
      " [ 1. 98.]\n",
      " [ 1. 99.]]\n"
     ]
    }
   ],
   "source": [
    "# Affichez le contenu de la nouvelle matrice X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez qu'à ce stade votre matrice $\\ X $ est de dimension $\\ 100 \\times 2 $ et que la première colonne colonne ne contient que des $\\ 1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation normale\n",
    "Dans le cas de la régression linéaire ($\\ \\hat{Y}= X \\cdot \\theta $ ), l'équation normale donne une solution exacte à la problématique de minimisation de la fonction ***Loss*** lorque celle-ci correspond à une MSE (Mean Squared Error).<br>\n",
    "\\begin{equation}\n",
    "\\theta=(X^T \\times X)^{-1} \\times X^T \\times Y\n",
    "\\end{equation}<br>\n",
    "Pour calculer les $\\ \\theta_{j} $, on utilise les méthodes de calcul matriciel de la bibliothèque ***numpy***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 5.]\n"
     ]
    }
   ],
   "source": [
    "Theta=np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507.0000000000002\n"
     ]
    }
   ],
   "source": [
    "# Votre code ici\n",
    "Y_101=np.dot(Theta,[1,101])\n",
    "print(Y_101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs des paramètres $\\ \\theta_{j} $ obtenues avec l'équation normale sont optimales. Néanmoins, le calcul peut être long lorsque le nombre de variables est grand. La descente de gradient est une alternative à l'équation normale dans le cas linéaire lorsque le nombre de variables est grand. \n",
    "Par ailleurs, la descente de gradient s'applique à des modèles non-linéaires pour lesuqles il n'existe pas de solution exacte à la problamatique de minimisation de la ***loss***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procédure générique de descente de gradient\n",
    "La descente de gradient est une approche ittérative qui conssiste à mettre à jour les pramètres $\\ \\theta_{j} $ en effectuant un déplacement dans le sens opposé au gradient.<br>\n",
    "De façon générique, la mise à jour des paramètres $\\ \\theta_{j} $ s’effectue comme suit :<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^k=\\theta^{k-1}- \\eta \\times \\nabla_{\\theta}MSE \n",
    "\\end{equation}<br>\n",
    "\n",
    "Avec :<br>\n",
    "- $\\ \\theta $ : le vecteur des paramètres $\\ \\theta_{j} $\n",
    "- $\\ k $ : le numéro d'itération en cours\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ \\nabla_{\\theta}MSE$ : le vecteur gradient <br>\n",
    "\n",
    "*Remarque: l'initialisation de $\\ \\theta $ est arbitraire.*<br>\n",
    "\n",
    "Il existe de nombreuse variantes de la descente de gradient qui se différentient par la façon de calculer $\\ \\nabla_{\\theta}MSE$. <br>\n",
    "Nous pouvons donc écrire une fonction générique de descente de gradient dont un des arguments est lui-même une fonction qui précise la façons de calculer $\\ \\nabla_{\\theta}MSE$.<br>\n",
    "Ecrvivez la fonction ***Grad_Desc*** qui renvoie en sortie $\\ \\theta $ et qui prend comme arguments:<br>\n",
    "- $\\ X $: la matrice des données d'apprentissage\n",
    "- $\\ Y $: le vecteur des labels\n",
    "- $\\ \\eta $ : le pas de gradient\n",
    "- $\\ K $ : le nombre total d'itérations\n",
    "- $\\ Grad $ : la fonction de calcul de $\\ \\nabla_{\\theta}MSE$ <br>\n",
    "\n",
    "*Remarque : dans un premier temps, faites comme si la fonction ***Grad*** était connue. Vous en préciserez le comportement dans un second temps.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc(X,Y,Eta,K,Grad):\n",
    "    # Init Theta as np array \n",
    "    # with dimension based on X shape    \n",
    "    Theta=np.array([1]*X.shape[1])\n",
    "    # Loop through each iteration\n",
    "    for i in range(K):\n",
    "        # Re-compute Theta during each \n",
    "        # iteration based on k-1 Theta\n",
    "        # and using the Grad function\n",
    "        Theta=Theta-Eta*Grad(X,Y,Theta)\n",
    "    # Return Theta array at the end of the loop\n",
    "    return(Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient ordinaire\n",
    "Les composantes du vecteur gradient correpondent aux dérivées partielles de la fonction ***Loss*** par rapport à chaque paramètre $\\ \\theta_{j} $. Le vecteur gradient s'écrit comme suit :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}MSE=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{0}}\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{1}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}\\\\\n",
    "\\ldots\\\\\n",
    "\\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{n}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Dans le cas de la descente de gradient ordinaire, la totalité du jeu d'entrainement est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous. <br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i=1}^{m} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= \\dfrac{2}{m} \\times X^T\\cdot(X\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Regular*** qui calcule le gradient ordinaire en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Regular(X,Y,Theta):\n",
    "    # m is based on the size of X\n",
    "    m = X.shape[0]\n",
    "    return (2/m)*X.transpose().dot(X.dot(Theta)-Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Regular***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06527489 5.01409242]\n"
     ]
    }
   ],
   "source": [
    "eta=0.0001\n",
    "K=100\n",
    "Regular_Theta=Grad_Desc(X,Y,eta,K,Grad_Regular)\n",
    "print(Regular_Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y prediction for x=101 using regular gradient descent: 507.49\n"
     ]
    }
   ],
   "source": [
    "# Votre code ici\n",
    "Regular_Raw_Y=np.dot(Regular_Theta,[1,101])\n",
    "Regular_Rounded_Y=round(Regular_Raw_Y,2)\n",
    "print(f\"Y prediction for x=101 using regular gradient descent: {Regular_Rounded_Y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient stochastique\n",
    "Dans le cas de la descente de gradient stochastique, seule une instance $\\ X_{i} $ du jeu d'entrainement, tirée au hasard à chaque itération, est utilisé pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ selon la formule ci-dessous.<br>\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=2 \\times x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Pour une instance $\\ i$ prise au hasard, le vecteur gradient $\\nabla_{\\theta}MSE $ peut s'obtenir directement à travers la formule matricielle: $\\nabla_{\\theta}MSE= 2 \\times X_{i}\\cdot(X_{i}\\cdot \\theta - Y) $\n",
    "\n",
    "Ecrivez la fonction ***Grad_Stochastic*** qui calcule le gradient stochastique en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def Grad_Stochastic(X,Y,Theta):\n",
    "    # m is based on the size of X\n",
    "    m=X.shape[0]\n",
    "    # i is a random integer in the range 0, m\n",
    "    # we use numpy instead of the random library \n",
    "    # to minimize import size as numpy provides the \n",
    "    # same function\n",
    "    i=np.random.randint(m)\n",
    "    # assign respectively, X[i] and Y[i] based \n",
    "    # on random array position i\n",
    "    Xi,Yi=X[i],Y[i]\n",
    "    # return the gradient computation\n",
    "    return 2*Xi.dot(Xi.dot(Theta)-Yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient stochastique sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Stochastic***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.07549561 5.0170083 ]\n"
     ]
    }
   ],
   "source": [
    "Eta=0.0001\n",
    "K=100\n",
    "Stochastic_Theta=Grad_Desc(X,Y,Eta,K,Grad_Stochastic)\n",
    "print(Stochastic_Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que constatez-vous au niveau des valeurs des paramètres $\\ \\theta_{j} $ ?<br>\n",
    "Expliquez le phénomène observé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y prediction for x=101 using stochastic gradient descent: 507.79\n"
     ]
    }
   ],
   "source": [
    "Stochastic_Raw_Y=np.dot(Stochastic_Theta,[1,101])\n",
    "Stochastic_Rounded_Y=round(Stochastic_Raw_Y,2)\n",
    "print(f\"Y prediction for x=101 using stochastic gradient descent: {Stochastic_Rounded_Y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descente de gradient mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas de la descente de gradient mini-batch, un sous-ensemble du jeu d'entrainement, constitué de $\\ q $ exemples tirés aléatoirement, est utilisé à chaque itération pour le calcul des $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$. Le nombre $\\ q $  est un hyperparamètre fixé par l'utilisateur avec $\\ q \\le m$.<br>\n",
    "\n",
    "La formule pour le calcul $\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ devient:\n",
    "\n",
    "$\\ \\dfrac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}=\\dfrac{2}{m}\\times \\sum_{i\\in \\mathscr{E}} x_{ij}\\times(X_{i} \\cdot \\theta - y_{i}) $<br>\n",
    "\n",
    "Avec $\\mathscr{E} $ l'ensemble des $\\ q $ exemples tirés aléatoirement (cet ensemble est réactualisé à cahque itération).\n",
    "\n",
    "Ecrivez la fonction ***Grad_Batch*** qui calcule le gradient mini_batch en $\\ \\theta $ et renvoie en sortie le vecteur gradient $\n",
    "\\nabla_{\\theta}MSE $<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Batch(X,Y,Theta,q):\n",
    "    # m is based on the size of X\n",
    "    m=X.shape[0]\n",
    "    # create empty list to store random numbers\n",
    "    # between 0 and m-1\n",
    "    Rand_Instances=[]\n",
    "    # while the length of the rand_instances \n",
    "    # list is inferior to the hyperparameter q\n",
    "    while len(Rand_Instances)<q:\n",
    "        # create a random integer\n",
    "        # between 0 and m-1\n",
    "        i=np.random.randint(m-1)\n",
    "        # if the number is not already\n",
    "        # in the rand_instances list\n",
    "        # push i at the end of it\n",
    "        # 'it allows us to have a \n",
    "        # unique array of \n",
    "        # gradient coefficients'\n",
    "        if i not in Rand_Instances:\n",
    "            Rand_Instances.append(i)\n",
    "    # when we have our list of rand_instances\n",
    "    # create an empty list of gradient coefficients\n",
    "    Grad_Coefficients=[]\n",
    "    # Loop through every k between 0 and the size\n",
    "    # of Theta\n",
    "    for k in range(len(Theta)):\n",
    "        # Compute the gradient coefficient for a given i and k\n",
    "        Coefficient=(2/m)*(sum([X[i][k]*(X[i].dot(Theta)-Y[i]) for i in Rand_Instances]))\n",
    "        # Add the coefficient to the list of coefficients\n",
    "        Grad_Coefficients.append(Coefficient)\n",
    "    # Return the list of gradient coefficients as np array\n",
    "    return np.array(Grad_Coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez la descente de gradient ordinaire sur votre jeu de donnée en utilisant la fonction ***Grad_Desc*** dont l'argument ***Grad*** est à remplacer par la fonction ***Grad_Batch***.\n",
    "Réalisez votre test avec :\n",
    "- $\\ \\eta =0.0001$\n",
    "- $\\ K=100$\n",
    "- $\\ q=10$\n",
    "\n",
    "*Remarque : pensez à adapter la fonction ***Grad_Desc*** en ***Grad_Desc_Batch***  pour tenir compte de l'hyperparamètre $\\ q $ spécifique à l'approche mini-batch. Dans la nouvelle fonction ***Grad_Desc_Batch***, le paramètre ***Grad*** prend par défaut la valeur ***Grad_Batch***.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Desc_Batch(X,Y,Eta,K,q,Grad=Grad_Batch):\n",
    "    # Initialize Theta with a list filled with 1's\n",
    "    # of the size of X \n",
    "    Theta=np.array([1]*X.shape[1])\n",
    "    # for i between 0 and K\n",
    "    for i in range(K):\n",
    "        # Compute Theta based on the k-1\n",
    "        # associated Theta value\n",
    "        # using the Mini-Batch method\n",
    "        Theta=Theta-Eta*Grad_Batch(X,Y,Theta,q)\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06098874 5.00869257]\n"
     ]
    }
   ],
   "source": [
    "Eta=0.0001\n",
    "K=100\n",
    "q=10\n",
    "MiniBatch_Theta=Grad_Desc_Batch(X,Y,Eta,K,q)\n",
    "print(MiniBatch_Theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant les $\\ \\theta_{j} $, réalisez une prédiction pour $\\ x = 101$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y prediction for x=101 using mini-batch gradient descent: 506.94\n"
     ]
    }
   ],
   "source": [
    "MiniBatch_Raw_Y=np.dot(MiniBatch_Theta,[1,101])\n",
    "MiniBatch_Rounded_Y=round(MiniBatch_Raw_Y,2)\n",
    "print(f\"Y prediction for x=101 using mini-batch gradient descent: {MiniBatch_Rounded_Y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests sur jeu de donnée plus conséquent\n",
    "Vous allez maintenant tester votre code sur un jeu de données plus conséquent et comparer vos résultats à ceux obtenus en utilisant la bibliothèque ***ScikitLearn***.<br>\n",
    "L'objectif est d'entrainer un régresseur linéaire qui prédit les dépenses de santé par ménage à partir de la base de données HISP (Health Insurance Subsidy Program). La base HISP contient des informations concernant les dépenses de santé de ménages américains avant et après la mise en plce du programme d'assurance HISP (les données sont fictives). <br>\n",
    "\n",
    "\n",
    "Pour des raisons de cohérence, vous n'utiliserez que les données décrivant la situation des ménages avant la mise en œuvre du programme HISP (rond=Before).<br>\n",
    "De plus, seules les variables continues ci-dessous seront utilisées pour entraîner votre modèle :\n",
    "- poverty_index \n",
    "- hhsize\n",
    "- age_sp\n",
    "- age_hh\n",
    "- educ_hh\n",
    "- educ_sp\n",
    "- hospital_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation et nettoyage des données\n",
    "Comme il s'agit simplement de comparer vos résultats avec ceux obtenus avec les regérsseurs ScikitLearn, il n'est pas nécessaire de séparer les données en jeu de test et jeu d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"datafinalexam.csv\") # importation des données (le fichier datafinalexam.csv et le notebook doivent être dans le même dossier)\n",
    "data=data.loc[data[\"round\"]==\"Before\"] # filtre pour garder la situation avant la mise en place du dispositif\n",
    "features=[\"poverty_index\",\"hhsize\",\"age_sp\",\"age_hh\",\"educ_hh\",\"educ_sp\",\"hospital_distance\"] # liste des variables à utiliser\n",
    "label=['health_expenditures'] # label\n",
    "data=data[features+label] # selction des colonnes utiles\n",
    "Train_set_features=data[features] # variables du jeu d'entrainement \n",
    "Train_set_label=data[label] # labeldu jeu d'entrainement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres\n",
    "eta=0.00001\n",
    "K=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=1e-05, learning_rate='constant')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de la classe SGDRegressor\n",
    "from sklearn.linear_model import SGDRegressor \n",
    "# Create a Stochastic Gradient Regressor model \n",
    "# with corresponding args\n",
    "reg = SGDRegressor(fit_intercept=True,learning_rate=\"constant\",eta0=eta,max_iter=K)\n",
    "# Fit our model with a feature and label training data sets\n",
    "reg.fit(Train_set_features,Train_set_label.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51223593  0.24923264 -1.34539748  0.02926478  0.12667713  0.25464952\n",
      "  0.2602398   0.00273023]\n"
     ]
    }
   ],
   "source": [
    "# Coefficient du modèle linéaire\n",
    "Theta=np.concatenate((reg.intercept_,reg.coef_),axis=None)\n",
    "print(Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.9813127469438\n"
     ]
    }
   ],
   "source": [
    "# Performance du modèle Scikit-Learn (RMSE sur jeu d'entrainement)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Prédiction faites par le modèle\n",
    "Pred=reg.predict(Train_set_features) \n",
    "# MSE entre prédictions et valeurs réelles\n",
    "MSE=mean_squared_error(Pred,Train_set_label) \n",
    "# RMSE \n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)\n",
    "# SGDRegressor from SciKitLearn\n",
    "# has a RMSE (Root Mean Squared Error) \n",
    "# that oscillate around 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement d'un régresseur linéaire avec descente de gradient stochastique - Votre approche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données dans un format compatible avec vos fonctions\n",
    "X=Stand_Trans(Train_set_features.values,Train_set_features.shape[0],Train_set_features.shape[1])\n",
    "Y=Train_set_label.values.reshape(Train_set_label.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions:(9913, 8)\n",
      "Y dimensions:(9913,)\n"
     ]
    }
   ],
   "source": [
    "# Affichez les dimensions de X\n",
    "print(f\"X dimensions:{X.shape}\")\n",
    "# Affichez les dimensions de Y\n",
    "print(f\"Y dimensions:{Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97789129  0.02313109  0.60106723  0.07512959  0.19841781  0.81546044\n",
      "  0.85736171 -0.00452756]\n"
     ]
    }
   ],
   "source": [
    "# Calcul des Theta\n",
    "Stochastic_Theta=Grad_Desc(X,Y,eta,K,Grad_Stochastic)\n",
    "print(Stochastic_Theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  7.889691630065824\n"
     ]
    }
   ],
   "source": [
    "# Performance de votre modèle (RMSE sur jeu d'entrainement)\n",
    "Pred=X.dot(Stochastic_Theta)\n",
    "MSE=mean_squared_error(Pred,Y)\n",
    "RMSE=MSE**(1/2)\n",
    "print(\"RMSE: \",RMSE)\n",
    "# Our model has a RMSE which oscillate\n",
    "# around 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison des résultats\n",
    "Comparer vos résultats à ceux obtenus avec ***SGDRegressor***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RMSE (standard deviation of the residual)\n",
    "# is higher when we use our model to predict Y\n",
    "# than when we use the scikit learn model\n",
    "# Therefore, the Stochastic Model from SciKit\n",
    "# perform better the prediction of Y\n",
    "# than our model\n",
    "\n",
    "# We can conclude, that it's important to \n",
    "# compare the RSME when we use different techniques\n",
    "# of gradient calculation, as the one with the lowest\n",
    "# RMSE is best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
